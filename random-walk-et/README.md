# ğŸ” Random Walk â€” Eligibility Traces (TD(Î»))

This folder is part of the  
**[Reinforcement-Learning](https://github.com/Meri-07m/Reinforcement-Learning)** repository by  
**[Meri-07m](https://github.com/Meri-07m)**.

It contains an **in-depth implementation and analysis** of **Eligibility Traces** applied to the classic **Random Walk** prediction problem.  
This is one of the most important examples in reinforcement learning theory, because it clearly demonstrates:

- how **Temporal Difference** learning works,  
- how **TD(Î»)** smoothly unifies TD(0) with Monte-Carlo methods,  
- how **credit assignment** propagates through sequences,  
- and how Î» influences the **speed and stability of learning**.

This module functions as both an **educational tutorial** and an **experimental playground** for deeply understanding TD(Î»).

---

# ğŸŒ 1. The Random Walk Environment

The classic 7-state random walk is a simple Markov Reward Process first introduced in RL literature as a clean environment to evaluate prediction algorithms.

### ğŸ“Œ Structure


- There are **5 non-terminal states** (Aâ€“E).
- The agent **starts in the middle state** (C).
- At every step:
  - With 0.5 probability â†’ move **left**
  - With 0.5 probability â†’ move **right**
- If the agent reaches:
  - **the right terminal** â†’ reward **+1**
  - **the left terminal** â†’ reward **0**
- The episode ends immediately when a terminal is reached.

Because the reward is only at termination, learning the value function is entirely about learning the **probability of reaching the right terminal from each state**.

---

# ğŸ§  2. True Values

For a symmetric random walk with start in the middle, the true values are known analytically:

| State | True Value |
|-------|------------|
| A | 1/6 |
| B | 2/6 |
| C | 3/6 |
| D | 4/6 |
| E | 5/6 |

These values are used to compute **RMS error curves**, which are a standard benchmark.

---

# âš¡ 3. Eligibility Traces: The Core Idea

Eligibility traces solve a fundamental problem in RL:

> **How much credit do we assign to earlier states when a TD error happens?**

Without traces, TD(0) gives credit *only* to the immediately preceding state.  
Monte-Carlo methods give credit *uniformly* to all states in the entire trajectory.

Eligibility traces provide a flexible middle ground.

---

# ğŸ” 4. What TD(Î») Does Conceptually

TD(Î») allocates credit backwards along the history of visited states:

- Recently visited states receive **high credit**
- Older states receive **decaying credit**
- The decay rate is controlled by **Î»**

### If Î» = 0  
Only the most recent state is updated â†’ **classic TD(0)**

### If Î» = 1  
All states in the episode get equal credit â†’ **Monte-Carlo** (every-visit)

### If 0 < Î» < 1  
A continuum of trade-offs:  

This dramatically improves learning efficiency.

---

# ğŸ”£ 5. Mathematical Details

## 5.1 Eligibility Trace Update

Each state \( s \) has a trace \( e(s) \).  
At timestep t:

### 1ï¸âƒ£ Increase trace for current state:
\[
e_t(s_t) \leftarrow e_t(s_t) + 1
\]

### 2ï¸âƒ£ Decay all traces:
\[
e_t(s) \leftarrow \gamma \lambda e_{t-1}(s)
\]

## 5.2 TD Error

\[
\delta_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t)
\]

## 5.3 Value Update

\[
V(s) \leftarrow V(s) + \alpha \, \delta_t \, e_t(s)
\]

This is the **backward view** of TD(Î»).

---

# ğŸ§© 6. Forward View vs Backward View

Eligibility traces have two mathematical perspectives:

### ğŸ“Œ Forward View  
Think in terms of **multi-step returns**:

\[
G_t^{(n)} = r_{t+1} + \gamma r_{t+2} + \dots + \gamma^{n-1} r_{t+n} + \gamma^n V(s_{t+n})
\]

TD(Î») uses a geometrically-weighted combination of all n-step returns.

### ğŸ“Œ Backward View  
The computational view:  
- Compute TD errors in real-time  
- Propagate them backward with eligibility traces

The backward view is **efficient** and equivalent to the forward view for linear function approximation.

---

# ğŸ§ª 7. Experiments Implemented

This module includes multiple experiments inspired by Sutton & Barto:

---

## ğŸ“˜ 7.1 Experiment: RMS Error Across Î»

A fundamental experiment that shows:

- how different Î» values affect prediction accuracy,
- measured across many episodes,
- averaged over many independent runs.

The famous result:

> **Intermediate values of Î» (around 0.7â€“0.9) perform best.**

---

## ğŸ“˜ 7.2 Experiment: Sensitivity to Î± (Learning Rate)

TD(Î») is sensitive to step size.  
For each Î», we test multiple Î± values and compute the average RMS error.

Result:

- Small Î» often allows **larger Î±**  
- Large Î» (close to 1) requires **very small Î±** for stability

---

## ğŸ“˜ 7.3 Experiment: Value Function Visualization

We generate plots such as:

- true value function
- predicted value function under different Î»
- learning curves over episodes
- evolution of state values over time

This helps visualize how trace propagation shapes learning.
                        



